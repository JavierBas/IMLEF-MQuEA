{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64kU3-ry_9yn"
      },
      "source": [
        "**Chapter 9 â€“ Unsupervised Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4olE7vi_9ys"
      },
      "source": [
        "_This notebook contains all the sample code and solutions to the exercises in chapter 9._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soGfK1FR_9yw"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/drive/1d79kHjKeC2eGqbUpzCOyHO0zNJurqTyo\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "eBJboRM5_9y0"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiKfPf47_9y9"
      },
      "source": [
        "As we did in previous topics, let's define the default font sizes to make the figures prettier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpwvnvWO_9zA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVdOFjsn_9zC"
      },
      "source": [
        "And let's create the `images/unsupervised_learning` folder (if it doesn't already exist), and define the `save_fig()` function which is used through this notebook to save the figures in high-res for the book:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP40TR79_9zD"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"unsupervised_learning\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JizE12oQ_9zF"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANxi7g3b_9zJ"
      },
      "source": [
        "## K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc9JfoqB_9zK"
      },
      "source": [
        "**Fit and predict**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw41fLmM_9zK"
      },
      "source": [
        "Let's train a K-Means clusterer on a dataset of blobs. \n",
        "`make_blobs()` is a function to create... well, blobs of points.\n",
        "\n",
        "K-means will try to find each blob's center and assign each instance to the closest blob:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1qZwngx_9zK"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "import numpy as np\n",
        "\n",
        "# This array defines the center of the blobs\n",
        "blob_centers = np.array([[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8],\n",
        "                         [-2.8,  2.8], [-2.8,  1.3]])\n",
        "# And this one the dispersion\n",
        "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
        "\n",
        "# Blobs creation!\n",
        "X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n",
        "                  random_state=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfJHF2lt_9zL"
      },
      "source": [
        "Now let's plot the blobs. This is not finding clusters, we are just plotting the observations that we have created (obviously they conform pretty identifiable groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxafIDNq_9zM"
      },
      "outputs": [],
      "source": [
        "def plot_clusters(X, y=None):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
        "    plt.xlabel(\"$x_1$\")\n",
        "    plt.ylabel(\"$x_2$\", rotation=0)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_clusters(X)\n",
        "plt.gca().set_axisbelow(True)\n",
        "plt.grid()\n",
        "save_fig(\"blobs_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's seems that there are 5 clearly separated groups (again, obvious, we made it on purpose). So let's make K-means find then 5 groups."
      ],
      "metadata": {
        "id": "rBsuDaDNIdx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)"
      ],
      "metadata": {
        "id": "sElkKqozGgnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnjhB8pO_9zM"
      },
      "source": [
        "when prediction, each instance i\n",
        "s assigned to one of the 5 clusters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZdtBn_D_9zN"
      },
      "outputs": [],
      "source": [
        "y_pred = kmeans.fit_predict(X)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the five centroids? Each centroid is defined by $x_1$ and $x_2$"
      ],
      "metadata": {
        "id": "wniOioaSI_fY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eilyV94H_9zO"
      },
      "outputs": [],
      "source": [
        "kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg61nmq3_9zO"
      },
      "source": [
        "We can also extrac the cluster each instance has been assigned to (remember that indices start at 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_662jdNq_9zP"
      },
      "outputs": [],
      "source": [
        "kmeans.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSSz-nBn_9zP"
      },
      "source": [
        "And, of course, we can predict the labels of new instances:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjhSrrC5_9zQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
        "kmeans.predict(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBhZWkcX_9zU"
      },
      "source": [
        "### The K-Means Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shmZwkpz_9zW"
      },
      "source": [
        "**K-Means Variability**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmlKP61a_9zX"
      },
      "source": [
        "In the original K-Means algorithm, the centroids are just initialized randomly, and the algorithm simply runs a single iteration to gradually improve the centroids.\n",
        "\n",
        "However, one major problem with this approach is that if you run K-Means multiple times (or with different random seeds), it can converge to very different solutions, as you can see below.\n",
        "\n",
        "**Don't feel overwhelmed** by this chunk of code. The important part is:\n",
        "```\n",
        "kmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=2)\n",
        "kmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=9)\n",
        "```\n",
        "the rest of the code is just plotting the results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugv7E6rz_9zX"
      },
      "outputs": [],
      "source": [
        "def plot_data(X):\n",
        "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
        "\n",
        "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
        "    if weights is not None:\n",
        "        centroids = centroids[weights > weights.max() / 10]\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                marker='o', s=35, linewidths=8,\n",
        "                color=circle_color, zorder=10, alpha=0.9)\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                marker='x', s=2, linewidths=12,\n",
        "                color=cross_color, zorder=11, alpha=1)\n",
        "\n",
        "\n",
        "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
        "                             show_xlabels=True, show_ylabels=True):\n",
        "    mins = X.min(axis=0) - 0.1\n",
        "    maxs = X.max(axis=0) + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
        "                         np.linspace(mins[1], maxs[1], resolution))\n",
        "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
        "                cmap=\"Pastel2\")\n",
        "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
        "                linewidths=1, colors='k')\n",
        "    plot_data(X)\n",
        "    if show_centroids:\n",
        "        plot_centroids(clusterer.cluster_centers_)\n",
        "\n",
        "    if show_xlabels:\n",
        "        plt.xlabel(\"$x_1$\")\n",
        "    else:\n",
        "        plt.tick_params(labelbottom=False)\n",
        "    if show_ylabels:\n",
        "        plt.ylabel(\"$x_2$\", rotation=0)\n",
        "    else:\n",
        "        plt.tick_params(labelleft=False)\n",
        "\n",
        "def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None,\n",
        "                              title2=None):\n",
        "    clusterer1.fit(X)\n",
        "    clusterer2.fit(X)\n",
        "\n",
        "    plt.figure(figsize=(10, 3.2))\n",
        "\n",
        "    plt.subplot(121)\n",
        "    plot_decision_boundaries(clusterer1, X)\n",
        "    if title1:\n",
        "        plt.title(title1)\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n",
        "    if title2:\n",
        "        plt.title(title2)\n",
        "\n",
        "kmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=2)\n",
        "kmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=9)\n",
        "\n",
        "plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,\n",
        "                          \"Solution 1\",\n",
        "                          \"Solution 2 (with a different random init)\")\n",
        "\n",
        "save_fig(\"kmeans_variability_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UszxaG0fNScF"
      },
      "outputs": [],
      "source": [
        "kmeans_rnd_init1.inertia_  # extra code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkJGPbnUNTYR"
      },
      "outputs": [],
      "source": [
        "kmeans_rnd_init1.inertia_  # extra code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If by any means (sorry, I couldn't help myself on doing this joke), we have prior information on where the centroids should be placed, we can indicate that to the algorithm."
      ],
      "metadata": {
        "id": "xsBTBUmWK7w7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QFF4z0i_9zY"
      },
      "outputs": [],
      "source": [
        "# This is information on where the centroids (supposedly) should be:\n",
        "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
        "\n",
        "# So we indicate them when fitting\n",
        "kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)\n",
        "kmeans.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See how with this initial information the clusters are much better captured"
      ],
      "metadata": {
        "id": "2ejBSnFMMr_1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYfesOyg_9zZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plot_decision_boundaries(kmeans, X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKx6VDsy_9za"
      },
      "source": [
        "### Inertia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnMQ4BLi_9zb"
      },
      "source": [
        "To select the best model, we will need a way to evaluate a K-Mean model's performance. Unfortunately, clustering is an unsupervised task, so we do not count with the targets. But at least we can measure the distance between each instance and its centroid. This is the idea behind the _inertia_ metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui13lHVU_9zb"
      },
      "outputs": [],
      "source": [
        "kmeans.inertia_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB82y4A5_9zc"
      },
      "outputs": [],
      "source": [
        "kmeans_rnd_init1.inertia_  # Remember that kmeans_rnd_init1 was a k-means fit with a different initialization that we estimated above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0vrgSFU_9zc"
      },
      "outputs": [],
      "source": [
        "kmeans_rnd_init2.inertia_  # Remember that kmeans_rnd_init2 was a k-means fit with a different initialization that we estimated above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ZyT91e_9ze"
      },
      "source": [
        "The `score()` method returns the negative inertia. Why negative? Well, it is because a predictor's `score()` method must always respect the \"_greater is better_\" rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKbZ2j3m_9ze"
      },
      "outputs": [],
      "source": [
        "kmeans.score(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnJOCgmm_9zf"
      },
      "source": [
        "### Multiple Initializations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQjKeE3g_9zf"
      },
      "source": [
        "So one approach to solve the variability issue is to simply run the K-Means algorithm multiple times with different random initializations, and select the solution that minimizes the inertia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZd6cVHg_9zf"
      },
      "source": [
        "When you set the `n_init` hyperparameter, Scikit-Learn runs the original algorithm `n_init` times, and selects the solution that minimizes the inertia. By default, Scikit-Learn sets `n_init=10`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwJNiV6__9zg"
      },
      "outputs": [],
      "source": [
        "kmeans_rnd_10_inits = KMeans(n_clusters=5, init=\"random\", n_init=10,\n",
        "                             random_state=2)\n",
        "kmeans_rnd_10_inits.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hv4L80-_9zg"
      },
      "source": [
        "As you can see, we end up with the initial model, which is certainly the optimal K-Means solution (at least in terms of inertia, and assuming $k=5$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TcyRqOW_9zg"
      },
      "outputs": [],
      "source": [
        "# extra code\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_decision_boundaries(kmeans_rnd_10_inits, X)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVif9Hs0_9zh"
      },
      "outputs": [],
      "source": [
        "kmeans_rnd_10_inits.inertia_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIPDWASX_9zq"
      },
      "source": [
        "### Finding the optimal number of clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNm_AzHl_9zr"
      },
      "source": [
        "What if the number of clusters was set to a lower or greater value than 5?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVc7-TJ1_9zr"
      },
      "outputs": [],
      "source": [
        "kmeans_k3 = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_k8 = KMeans(n_clusters=8, random_state=42)\n",
        "\n",
        "plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, \"$k=3$\", \"$k=8$\")\n",
        "save_fig(\"bad_n_clusters_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1If4ndMg_9zr"
      },
      "source": [
        "Ouch, these two models don't look great. What about their inertias?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_LqsnLq_9zs"
      },
      "outputs": [],
      "source": [
        "kmeans_k3.inertia_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wKly81T_9zt"
      },
      "outputs": [],
      "source": [
        "kmeans_k8.inertia_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7Y9BnYq_9zu"
      },
      "source": [
        "This is proof that **we cannot simply take the value of $k$ that minimizes the inertia, since it keeps getting lower as we increase $k$** (the more clusters there are, the closer each instance will be to its closest centroid, and therefore the lower the inertia will be). \n",
        "\n",
        "However, we can plot the inertia as a function of $k$ and analyze the resulting curve:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIsVwrWE_9zv"
      },
      "outputs": [],
      "source": [
        "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n",
        "                for k in range(1, 10)]\n",
        "inertias = [model.inertia_ for model in kmeans_per_k]\n",
        "\n",
        "plt.figure(figsize=(8, 3.5))\n",
        "plt.plot(range(1, 10), inertias, \"bo-\")\n",
        "plt.xlabel(\"$k$\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.annotate(\"\", xy=(4, inertias[3]), xytext=(4.45, 650),\n",
        "             arrowprops=dict(facecolor='black', shrink=0.1))\n",
        "plt.text(4.5, 650, \"Elbow\", horizontalalignment=\"center\")\n",
        "plt.axis([1, 8.5, 0, 1300])\n",
        "plt.grid()\n",
        "save_fig(\"inertia_vs_k_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vm66YjA_9zw"
      },
      "source": [
        "As you can see, there is an elbow at $k=4$, which means that less clusters than that would be bad, and more clusters would not help much and might cut clusters in half. So $k=4$ is a pretty good choice. \n",
        "\n",
        "Note that the two blobs in the lower left will be considered as just a single cluster, so it may also make sense to define $k=5$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLvg4iAy_9zy"
      },
      "outputs": [],
      "source": [
        "# extra code\n",
        "plot_decision_boundaries(kmeans_per_k[4 - 1], X)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clkMhLv0_9zz"
      },
      "source": [
        "So another approach (or just if are looking for confirmation) is to look at the _silhouette score_, which is the mean _silhouette coefficient_ over all the instances. \n",
        "\n",
        "Remember that an instance's silhouette coefficient is equal to (_b_ - _a_) / max(_a_, _b_) where _a_ is the mean distance to the other instances in the same cluster (it is the _mean intra-cluster distance_), and _b_ is the _mean nearest-cluster distance_, that is the mean distance to the instances of the next closest cluster (defined as the one that minimizes _b_, excluding the instance's own cluster). \n",
        "\n",
        "The silhouette coefficient can vary between -1 and +1: \n",
        "- A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters\n",
        "- A coefficient close to 0 means that it is close to a cluster boundary, \n",
        "- A coefficient close to -1 means that the instance may have been assigned to the wrong cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBuKQhSO_9zz"
      },
      "source": [
        "Let's plot the silhouette score as a function of $k$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgYqvUnh_9z0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "silhouette_score(X, kmeans.labels_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvoBCth__9z1"
      },
      "outputs": [],
      "source": [
        "silhouette_scores = [silhouette_score(X, model.labels_)\n",
        "                     for model in kmeans_per_k[1:]]\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
        "plt.xlabel(\"$k$\")\n",
        "plt.ylabel(\"Silhouette score\")\n",
        "plt.axis([1.8, 8.5, 0.55, 0.7])\n",
        "plt.grid()\n",
        "save_fig(\"silhouette_score_vs_k_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j1OY858_9z2"
      },
      "source": [
        "As we can see, this visualization is much richer than the previous one: in particular, although it confirms that $k=4$ is a very good choice, but it also underlines the fact that $k=5$ may be quite good as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqHIg_1b_9z2"
      },
      "source": [
        "An even more informative visualization is given when we plot every instance's silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. This is called a _silhouette diagram_.\n",
        "\n",
        "Again, do not feel overwhelmed by the code. The important part is:\n",
        "```\n",
        "y_pred = kmeans_per_k[k - 1].labels_\n",
        "silhouette_coefficients = silhouette_samples(X, y_pred)\n",
        "```\n",
        "the rest if just for creating and giving format to the plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlrvTs4c_9z2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_samples\n",
        "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
        "\n",
        "plt.figure(figsize=(11, 9))\n",
        "\n",
        "for k in (3, 4, 5, 6):\n",
        "    plt.subplot(2, 2, k - 2)\n",
        "    \n",
        "    y_pred = kmeans_per_k[k - 1].labels_\n",
        "    silhouette_coefficients = silhouette_samples(X, y_pred)\n",
        "\n",
        "    padding = len(X) // 30\n",
        "    pos = padding\n",
        "    ticks = []\n",
        "    for i in range(k):\n",
        "        coeffs = silhouette_coefficients[y_pred == i]\n",
        "        coeffs.sort()\n",
        "\n",
        "        color = plt.cm.Spectral(i / k)\n",
        "        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "        ticks.append(pos + len(coeffs) // 2)\n",
        "        pos += len(coeffs) + padding\n",
        "\n",
        "    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
        "    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n",
        "    if k in (3, 5):\n",
        "        plt.ylabel(\"Cluster\")\n",
        "    \n",
        "    if k in (5, 6):\n",
        "        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "        plt.xlabel(\"Silhouette Coefficient\")\n",
        "    else:\n",
        "        plt.tick_params(labelbottom=False)\n",
        "\n",
        "    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n",
        "    plt.title(f\"$k={k}$\")\n",
        "\n",
        "save_fig(\"silhouette_analysis_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other clustering algorithms"
      ],
      "metadata": {
        "id": "jcaTMVTHSQue"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv8s74-a_90M"
      },
      "source": [
        "## DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN fins high-density portions of the feature space, grouping the observations that fall on them. Therefore, it works well if all the clusters are dense enough and if they are well spearated by low-density regions."
      ],
      "metadata": {
        "id": "4zsXPesWShUG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB_wsgpI_90N"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)\n",
        "dbscan = DBSCAN(eps=0.05, min_samples=5) # eps is the maximum distance for two observations to be considered 'neighbors'\n",
        "dbscan.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a label of -1 means that the instance has been considered an _anomaly_"
      ],
      "metadata": {
        "id": "VWjqUGsTTXAN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXCNR3I1_90N"
      },
      "outputs": [],
      "source": [
        "dbscan.labels_[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run the analysis with eps =0.2, too, and then plot the data and the clusters found in both cases."
      ],
      "metadata": {
        "id": "7ZOkRQmMTgPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej-8aOPF_90P"
      },
      "outputs": [],
      "source": [
        "def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n",
        "    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
        "    core_mask[dbscan.core_sample_indices_] = True\n",
        "    anomalies_mask = dbscan.labels_ == -1\n",
        "    non_core_mask = ~(core_mask | anomalies_mask)\n",
        "\n",
        "    cores = dbscan.components_\n",
        "    anomalies = X[anomalies_mask]\n",
        "    non_cores = X[non_core_mask]\n",
        "    \n",
        "    plt.scatter(cores[:, 0], cores[:, 1],\n",
        "                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n",
        "    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n",
        "                c=dbscan.labels_[core_mask])\n",
        "    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n",
        "                c=\"r\", marker=\"x\", s=100)\n",
        "    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n",
        "                c=dbscan.labels_[non_core_mask], marker=\".\")\n",
        "    if show_xlabels:\n",
        "        plt.xlabel(\"$x_1$\")\n",
        "    else:\n",
        "        plt.tick_params(labelbottom=False)\n",
        "    if show_ylabels:\n",
        "        plt.ylabel(\"$x_2$\", rotation=0)\n",
        "    else:\n",
        "        plt.tick_params(labelleft=False)\n",
        "    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n",
        "    plt.grid()\n",
        "    plt.gca().set_axisbelow(True)\n",
        "\n",
        "dbscan2 = DBSCAN(eps=0.2)\n",
        "dbscan2.fit(X)\n",
        "\n",
        "plt.figure(figsize=(9, 3.2))\n",
        "\n",
        "plt.subplot(121)\n",
        "plot_dbscan(dbscan, X, size=100)\n",
        "\n",
        "plt.subplot(122)\n",
        "plot_dbscan(dbscan2, X, size=600, show_ylabels=False)\n",
        "\n",
        "save_fig(\"dbscan_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With eps = 0.5, 7 clusters are found, with a lot of anomalies (red crosses).\n",
        "However, with eps = 0.2, we get a much better categorizartion."
      ],
      "metadata": {
        "id": "RLhvhaskombm"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}